---
title: "PracticalMachineLearning_Proj V2"
output: html_document
---
# Synopsis
This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
#Background

##Data

##Study objective

#Author notes

#Study and results
##Loading Packages
```{r Packages, results='hide'}
library(caret)
library(ggplot2)
library(dplyr)
library(rattle)
library(rpart)
library(rpart.plot)
```
##Initializations
```{r Initializations}
rm(list=ls())
# setting the seed generator number for rerpoducability
set.seed(1235)
```
##Loading training and testing data
```{r Data_Loading, cache=TRUE}
# Downloading the training data
#trainingFile.url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
#trainingFile=download.file(url=trainingFile.url, destfile=trainingDestfile)
trainingDestfile=".//pml-training.csv"
trainingDataRaw = read.csv(file=trainingDestfile)

# Downloading the testing data
#testingFile.url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#testingFile=download.file(url=testingFile.url, destfile=testingDestfile)
testingDestfile=".//pml-testing.csv"
testingDataRaw = read.csv(file=testingDestfile)

rm(trainingDestfile, testingDestfile)

#Total number of variables 160 after this stage
```

##Data Processing
###Data Cleaning
```{r Data_Cleaning, cache=TRUE}
# Cleaning the training set
# deleting columns with NAs
trainingDataClean = trainingDataRaw[,colSums(is.na(trainingDataRaw)) == 0]
# Columns 1-7 are irrelevant to our study
irrelevant_indices = 1:7
relevant_indices = - irrelevant_indices
trainingDataClean = trainingDataClean[,relevant_indices]

# Cleaning the testing set
# deleting columns with NAs
testingDataClean = testingDataRaw[,colSums(is.na(testingDataRaw)) == 0]
# Columns 1-7 are irrelevant to our study
irrelevant_indices = 1:7
relevant_indices = - irrelevant_indices
testingDataClean = testingDataClean[,relevant_indices]
rm(trainingDataRaw, testingDataRaw, irrelevant_indices, relevant_indices)
#Total number of variables after this stage, training set: 86 and for intermediate testing: N/A #Testing set: 53
```

###Data Splitting
```{r Data_Splitting, cache=TRUE}
inTrain = createDataPartition(y=trainingDataClean$classe, p=0.75, list= FALSE)
training = trainingDataClean[inTrain,]
testing = trainingDataClean[-inTrain,]

dim(training); dim(testing)

# Initializing the variables for the data to be used in training
# Data Alternative A
trainingStep = training
# Data Alternative B
trainingStepB = training
rm(training, inTrain)

#Total number of variables after this stage, training set A 86 / training set B 86
#Testing set: 53
```
###Exploratory Data Analysis
```{r}

```
###Removing zero- and near-zero-variance variables
```{r ZeroVarianceVariables, cache=TRUE}
# Removing variables that have zero or near-zero variance
nzv = nearZeroVar(trainingStep, saveMetrics = TRUE)
trainingNZV = trainingStep[, !nzv$zeroVar & !nzv$nzv ]
dim(trainingNZV)
trainingStep = trainingNZV
rm(nzv, trainingNZV)

nzvB = nearZeroVar(trainingStepB, saveMetrics = TRUE)
trainingNZVB = trainingStepB[, !nzvB$zeroVar & !nzvB$nzv ]
dim(trainingNZVB)
trainingStepB = trainingNZVB
rm(nzvB, trainingNZVB)

#Total number of variables after this stage, training set A 53 / training set B 53
#Testing set: 53
```
###Removing highly correlated variables
```{r Removing_Correlated_Vars, cache=TRUE}
# Removing highly correlated variables
# training data set A
# removing the classe variable
trainingTemp = trainingStep[,-53]
descrCor <- cor(trainingTemp)
highlyCorrIndices = findCorrelation(descrCor, cutoff = 0.75)
trainingCor = trainingTemp[,-highlyCorrIndices]
# adding the classe variable back
trainingCor$classe = trainingStep$classe
trainingStep = trainingCor

# training data set B
# removing the classe variable
trainingTempB = trainingStepB[,-53]
descrCorB <- cor(trainingTempB)
highlyCorrIndicesB = findCorrelation(descrCorB, cutoff = 0.75)
trainingCorB = trainingTempB[,-highlyCorrIndicesB]
# adding the classe variable back
trainingCorB$classe = trainingStepB$classe
trainingStepB = trainingCorB

rm(trainingCor, trainingCorB, trainingTemp, trainingTempB, descrCor, descrCorB, highlyCorrIndices, highlyCorrIndicesB)

#Total number of variables after this stage, training set A 33 / training set B 33
#Testing set: 53
```
##Choosing final training data
```{r Final_Training_Data, cache=TRUE}
# Training data A for decision trees with no cross-validation
trainingSemifinal = trainingStep

# Training data B for decision trees with 10-fold cross-validation repeated 2 times
trainingSemifinalB = trainingStepB

# Training data C for random forests with 10-fold cross-validation repeated 2 times
trainingSemifinalC = trainingStepB

rm(trainingStep, trainingStepB)

#Total number of variables after this stage, training set A 33 / training set B 33 / 
#training set C 33
#Testing set: 53
```

##Training
###Training using decision trees with no cross-validation
####Training Process
```{r Training_DT_without_cv, cache=TRUE }
modelFit <- train(classe ~ .,method="rpart",data=trainingSemifinal)
fancyRpartPlot(modelFit$finalModel)
```
####Pedictions
```{r Predictiions_DT_without_cv, cache=TRUE}
predicted = predict(modelFit, newdata=testing)
confusionMatrix(predicted, testing$classe)
```
###Training using decision trees with 10-fold cross-validation repeated 10 times
####Training Process
```{r Training_DT_10-fold_cv_rep_2_times, cache=TRUE}
ctrlB = trainControl(method="repeatedcv", number = 10, repeats = 2)
modelFitB <- train(classe ~., data = trainingSemifinalB, method="rpart", tuneLength = 15, trControl=ctrlB)
fancyRpartPlot(modelFitB$finalModel)
```
####Pedictions
```{r Predictions_DT_10-fold_cv_rep_2_times, cache=TRUE}
predictedB = predict(modelFitB, newdata=testing)
confusionMatrix(predictedB, testing$classe)
```
###Training using random forests with 3-fold cross-validation repeated 2 times
####Training Process
```{r Training_RF_3-fold_cv_rep_2_times, cache=TRUE}
# Too much time for 10-cv with 10 repetitions
#ctrlC = trainControl(method="repeatedcv", number = 10, repeats = 10)
#modelFitC <- train(classe ~., data = trainingSemifinalB, method="rf", tuneLength = 10, trControl=ctrlC, verbose = TRUE, trace = TRUE)

# Moderate acceptable time for 3-cv with 2 repetitions
ctrlC = trainControl(method="repeatedcv", number = 3, repeats = 2)
modelFitC <- train(classe ~., data = trainingSemifinalC, method="rf", tuneLength = 2, trControl=ctrlC)
```
####Predictions
```{r Predictions_RF_3-fold_cv_rep_2_times, cache=TRUE}
predictedC = predict(modelFitC, newdata=testing)
confusionMatrix(predictedC, testing$classe)
```
##Writing answers to files
```{r Writing_the_answers, cache=TRUE}
answers = predict(modelFitC, newdata=testingDataClean)
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)
```
##Results

###Model Evaluation

###Out of sample error


#Conclusion
